{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"D2b Data Pipeline Overview D2b is a simple data pipeline designed to help automate the processes involved in extracting, transforming, analysing and exporting data insights carried out by data professionals at Data2bot. The automation pipeline is designed to abstract complexities and allow analysts to focus solely on SQL. Key Implementation Tools \u2705 Python \u2705 Postgresql \u2705 Makedoc \u2705 GNU Makefile \u2705 Github Actions Installation and setup \ud83d\udd29\ud83e\ude9b Clone the repository. git clone https://github.com/anochima/data2bot.git cd data2bot make setup The above commands: Creates and activate a virtual environmnent (.data2bot) at the root directory Installs all neccessary packages needed to successfully run the project And finally creates a configuration file (config.ini) for setting up the Database connections, etc. Database Configuration \ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb After running the above script, a new configuration file will be added to the project directory config.ini . Make sure to set up all necessary configurations for the database. Note \u2139\ufe0f: The config.ini file is intended to abstract valuable information regarding database connection. Hence, informations added here will be ignored during deployment. [SERVER] DB_CONNECTION=pgsql DB_HOST=localhost DB_PORT=5432 DB_DATABASE=postgres DB_USERNAME=root DB_PASSWORD='' DB_DEFAULT_SCHEMA = DB_STAGING_SCHEMA = DB_ANALYTICS_SCHEMA = S3_WAREHOUSE_BUCKET_NAME = Retrieving configuration data from config.ini To access the configuration variables into your python script. Import env function from handlers.env_handler . The env() function sets or returns config file section, key value env('SECTION', 'KEY', 'VALUE') . section: The config file section e.g SERVER key: A key in the selected section value:(str, optional) If set, overides the existing section key value in config.ini and set new key to the value specified. # scripts/Handlers/env_handler.py from handlers.env_handler import env # Get the database username username = env ( 'SERVER' , 'DB_USERNAME' ) print ( username ) #output: root # Change the username from script new_username = env ( 'SERVER' , 'DB_USERNAME' , 'user' ) print ( new_username ) #output: user Importing Data \ud83c\udfec There are 2 ways to import data currently; [\"DB\", \"WAREHOUSE\"] # /scripts/start.py from Providers.ImportDataServiceProvider import ImportDataServiceProvider import_service = ImportDataServiceProvider ( import_from = \"WAREHOUSE\" ) ''' Specify where to import where to import the files, either 'DB' or 'WAREHOUSE'. If not set, defaults to \"DB\" ''' # A list of files/object names to import import_service . service_list = [ \"orders.csv\" , \"reviews.csv\" , \"shipment_deliveries.csv\" , ] # start import import_service . execute_service () These \"import froms\" can be modified in the config.ini configuration file. DATA_STORES = [\"DB\", \"WAREHOUSE\"] Exporting Data \u2b06\ufe0f # /scripts/start.py from Providers.ExportDataServiceProvider import ExportDataServiceProvider export_service = ExportDataServiceProvider ( export_to = \"DB\" ) ''' Specify where to export the files to, either 'DB' or 'WAREHOUSE'. If not set, defaults to \"DB\" ''' # A list of files/object names to export export_service . service_list = [ \"../data2bot/data/raw/orders.csv\" , \"../data2bot/data/raw/reviews.csv\" , \"../data2bot/data/raw/shipment_deliveries.csv\" , ] # start the export export_service . execute_service () # /scripts/start.py from Providers.ExportDataServiceProvider import ExportDataServiceProvider export_service = ExportDataServiceProvider ( export_to = \"WAREHOUSE\" ) ''' Specify where to export the files to, either 'DB' or 'WAREHOUSE'. If not set, defaults to \"DB\" ''' # A list of files/object names to export export_service . service_list = [ \"../data2bot/data/transformed/orders.csv\" , \"../data2bot/data/raw/reviews.csv\" , \"../data2bot/data/raw/shipment_deliveries.csv\" , ] # start the export export_service . execute_service () Running SQL Queries All external SQL queries are stored inside the /SQL directory. For an external query to be executed, it must be registered inside the Analytics Service Provider class. # scripts/start.py from Providers.AnalyticsServiceProvider import AnalyticsServiceProvider analytics_service = AnalyticsServiceProvider () # list of analytics scripts in /sql to run e.g. \"product_analysis.sql\" analytics_service . service_list = [ \"../data2bot/sql/product_orders_on_holidays.sql\" , \"../data2bot/sql/total_late_and_undelivered_shipments.sql\" , \"../data2bot/sql/product_reviews_analytics.sql\" , ] # start running analysis analytics_service . execute_service () Running the Pipeline \u26a1\ufe0f To run the pipeline, simply run the following command in your terminal. make run Documentation To read the documentation, run mkdocs serve on terminal","title":"\ud83c\udfe0 &nbsp; Home"},{"location":"#d2b-data-pipeline","text":"","title":"D2b Data Pipeline"},{"location":"#overview","text":"D2b is a simple data pipeline designed to help automate the processes involved in extracting, transforming, analysing and exporting data insights carried out by data professionals at Data2bot. The automation pipeline is designed to abstract complexities and allow analysts to focus solely on SQL.","title":"Overview"},{"location":"#key-implementation-tools","text":"\u2705 Python \u2705 Postgresql \u2705 Makedoc \u2705 GNU Makefile \u2705 Github Actions","title":"Key Implementation Tools"},{"location":"#installation-and-setup","text":"Clone the repository. git clone https://github.com/anochima/data2bot.git cd data2bot make setup The above commands: Creates and activate a virtual environmnent (.data2bot) at the root directory Installs all neccessary packages needed to successfully run the project And finally creates a configuration file (config.ini) for setting up the Database connections, etc.","title":"Installation and setup \ud83d\udd29\ud83e\ude9b"},{"location":"#database-configuration","text":"After running the above script, a new configuration file will be added to the project directory config.ini . Make sure to set up all necessary configurations for the database. Note \u2139\ufe0f: The config.ini file is intended to abstract valuable information regarding database connection. Hence, informations added here will be ignored during deployment. [SERVER] DB_CONNECTION=pgsql DB_HOST=localhost DB_PORT=5432 DB_DATABASE=postgres DB_USERNAME=root DB_PASSWORD='' DB_DEFAULT_SCHEMA = DB_STAGING_SCHEMA = DB_ANALYTICS_SCHEMA = S3_WAREHOUSE_BUCKET_NAME =","title":"Database Configuration \ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb"},{"location":"#retrieving-configuration-data-from-configini","text":"To access the configuration variables into your python script. Import env function from handlers.env_handler . The env() function sets or returns config file section, key value env('SECTION', 'KEY', 'VALUE') . section: The config file section e.g SERVER key: A key in the selected section value:(str, optional) If set, overides the existing section key value in config.ini and set new key to the value specified. # scripts/Handlers/env_handler.py from handlers.env_handler import env # Get the database username username = env ( 'SERVER' , 'DB_USERNAME' ) print ( username ) #output: root # Change the username from script new_username = env ( 'SERVER' , 'DB_USERNAME' , 'user' ) print ( new_username ) #output: user","title":"Retrieving configuration data from config.ini"},{"location":"#importing-data","text":"There are 2 ways to import data currently; [\"DB\", \"WAREHOUSE\"] # /scripts/start.py from Providers.ImportDataServiceProvider import ImportDataServiceProvider import_service = ImportDataServiceProvider ( import_from = \"WAREHOUSE\" ) ''' Specify where to import where to import the files, either 'DB' or 'WAREHOUSE'. If not set, defaults to \"DB\" ''' # A list of files/object names to import import_service . service_list = [ \"orders.csv\" , \"reviews.csv\" , \"shipment_deliveries.csv\" , ] # start import import_service . execute_service () These \"import froms\" can be modified in the config.ini configuration file. DATA_STORES = [\"DB\", \"WAREHOUSE\"]","title":"Importing Data \ud83c\udfec"},{"location":"#exporting-data","text":"# /scripts/start.py from Providers.ExportDataServiceProvider import ExportDataServiceProvider export_service = ExportDataServiceProvider ( export_to = \"DB\" ) ''' Specify where to export the files to, either 'DB' or 'WAREHOUSE'. If not set, defaults to \"DB\" ''' # A list of files/object names to export export_service . service_list = [ \"../data2bot/data/raw/orders.csv\" , \"../data2bot/data/raw/reviews.csv\" , \"../data2bot/data/raw/shipment_deliveries.csv\" , ] # start the export export_service . execute_service () # /scripts/start.py from Providers.ExportDataServiceProvider import ExportDataServiceProvider export_service = ExportDataServiceProvider ( export_to = \"WAREHOUSE\" ) ''' Specify where to export the files to, either 'DB' or 'WAREHOUSE'. If not set, defaults to \"DB\" ''' # A list of files/object names to export export_service . service_list = [ \"../data2bot/data/transformed/orders.csv\" , \"../data2bot/data/raw/reviews.csv\" , \"../data2bot/data/raw/shipment_deliveries.csv\" , ] # start the export export_service . execute_service ()","title":"Exporting Data \u2b06\ufe0f"},{"location":"#running-sql-queries","text":"All external SQL queries are stored inside the /SQL directory. For an external query to be executed, it must be registered inside the Analytics Service Provider class. # scripts/start.py from Providers.AnalyticsServiceProvider import AnalyticsServiceProvider analytics_service = AnalyticsServiceProvider () # list of analytics scripts in /sql to run e.g. \"product_analysis.sql\" analytics_service . service_list = [ \"../data2bot/sql/product_orders_on_holidays.sql\" , \"../data2bot/sql/total_late_and_undelivered_shipments.sql\" , \"../data2bot/sql/product_reviews_analytics.sql\" , ] # start running analysis analytics_service . execute_service ()","title":"Running SQL Queries"},{"location":"#running-the-pipeline","text":"To run the pipeline, simply run the following command in your terminal. make run","title":"Running the Pipeline \u26a1\ufe0f"},{"location":"#documentation","text":"To read the documentation, run mkdocs serve on terminal","title":"Documentation"},{"location":"handlers/","text":"Handlers Database Connection Handler The database handler establishes necessary connection with the database based on information provided in the config.ini file. All errors encountered during the course of connection are logged to logs/data2bot.log class DatabaseConn : def __init__ ( self , connector = env ( \"SERVER\" , \"DB_CONNECTOR\" )) -> None : \"\"\"Establish database connection based on the server settings in config.ini Args: log_file (str, optional): The name or path to the file where error will be logged. Defaults to ERROR_LOG path in config.ini connection (str, optional): The database connector, Default to DB_CONNECTOR in config.ini \"\"\" # Load connection variables from config.ini self . DB_HOST = env ( \"SERVER\" , \"DB_HOST\" ) self . DB_PORT = env ( \"SERVER\" , \"DB_PORT\" ) self . DB_DATABASE = env ( \"SERVER\" , \"DB_DATABASE\" ) self . DB_USERNAME = env ( \"SERVER\" , \"DB_USERNAME\" ) self . DB_PASSWORD = env ( \"SERVER\" , \"DB_PASSWORD\" ) self . connector = connector def __alchemy ( self ): return create_engine ( f \"postgresql+psycopg2:// { self . DB_USERNAME } : { self . DB_PASSWORD } @ { self . DB_HOST } / { self . DB_DATABASE } \" ) def __pg ( self ): # Establish a connection with the db try : conn = pg . connect ( host = self . DB_HOST , dbname = self . DB_DATABASE , user = self . DB_USERNAME , password = self . DB_PASSWORD , port = self . DB_PORT , ) return conn except Exception as e : print ( e ) def connect ( self ): conn = None if self . connector . lower () == \"alchemy\" : conn = self . __alchemy () elif self . connector . lower () == \"pg\" : conn = self . __pg () else : raise TypeError ( \"Invalid connector, expects either pg or alchemy\" ) return conn Env Connection Handler # scripts/handlers/env_handler.py # read configuraton file config_file = \"../data2bot/config.ini\" config = ConfigParser () config . read ( config_file ) def env ( section , key , value = None ): \"\"\" Sets or returns config file section, key value parameters ---------- section: The config file section key: A key in the selected section value: desired value for the selected key, if not set, returns the key's current value. \"\"\" if value is None : return config [ section ][ key ] else : config [ section ][ key ] = value return config Service Handler class Service ( ABC ): service_list = None service_path = None def __init_subclass__ ( cls , ** kwargs ) -> None : for required in ( \"service_list\" , \"service_path\" ): if not getattr ( cls , required ): raise TypeError ( f \"Can't instantiate class { cls . __name__ } without { required } attribute defined\" ) return super () . __init_subclass__ ( ** kwargs ) @abstractclassmethod def services ( self , * args , ** kwargs ): return [ \"\" . join ([ self . service_path , service ]) for service in self . service_list ] @abstractclassmethod def execute_service ( self , * args , ** kwargs ): return None Log Handler The log handler control how logs are formatted. from Handlers.env_handler import env class LogHandler : def __init__ ( self , log_file = env ( \"LOG\" , \"ERROR_LOG\" )): # set our database logger self . logger = logging . getLogger ( __name__ ) self . logger . setLevel ( env ( \"LOG\" , \"LOG_LEVEL\" )) # set the log formatter self . formatter = logging . Formatter ( \"------------------------ \\n %(asctime)s \\n ------------------------ \\n %(filename)s : \\n \\ \\n %(message)s \\n @ %(funcName)s : %(pathname)s \" ) # set the file handler self . file_handler = logging . FileHandler ( filename = log_file ) self . file_handler . setFormatter ( self . formatter ) self . logger . addHandler ( self . file_handler )","title":"**Handlers**"},{"location":"handlers/#handlers","text":"","title":"Handlers"},{"location":"handlers/#database-connection-handler","text":"The database handler establishes necessary connection with the database based on information provided in the config.ini file. All errors encountered during the course of connection are logged to logs/data2bot.log class DatabaseConn : def __init__ ( self , connector = env ( \"SERVER\" , \"DB_CONNECTOR\" )) -> None : \"\"\"Establish database connection based on the server settings in config.ini Args: log_file (str, optional): The name or path to the file where error will be logged. Defaults to ERROR_LOG path in config.ini connection (str, optional): The database connector, Default to DB_CONNECTOR in config.ini \"\"\" # Load connection variables from config.ini self . DB_HOST = env ( \"SERVER\" , \"DB_HOST\" ) self . DB_PORT = env ( \"SERVER\" , \"DB_PORT\" ) self . DB_DATABASE = env ( \"SERVER\" , \"DB_DATABASE\" ) self . DB_USERNAME = env ( \"SERVER\" , \"DB_USERNAME\" ) self . DB_PASSWORD = env ( \"SERVER\" , \"DB_PASSWORD\" ) self . connector = connector def __alchemy ( self ): return create_engine ( f \"postgresql+psycopg2:// { self . DB_USERNAME } : { self . DB_PASSWORD } @ { self . DB_HOST } / { self . DB_DATABASE } \" ) def __pg ( self ): # Establish a connection with the db try : conn = pg . connect ( host = self . DB_HOST , dbname = self . DB_DATABASE , user = self . DB_USERNAME , password = self . DB_PASSWORD , port = self . DB_PORT , ) return conn except Exception as e : print ( e ) def connect ( self ): conn = None if self . connector . lower () == \"alchemy\" : conn = self . __alchemy () elif self . connector . lower () == \"pg\" : conn = self . __pg () else : raise TypeError ( \"Invalid connector, expects either pg or alchemy\" ) return conn","title":"Database Connection Handler"},{"location":"handlers/#env-connection-handler","text":"# scripts/handlers/env_handler.py # read configuraton file config_file = \"../data2bot/config.ini\" config = ConfigParser () config . read ( config_file ) def env ( section , key , value = None ): \"\"\" Sets or returns config file section, key value parameters ---------- section: The config file section key: A key in the selected section value: desired value for the selected key, if not set, returns the key's current value. \"\"\" if value is None : return config [ section ][ key ] else : config [ section ][ key ] = value return config","title":"Env Connection Handler"},{"location":"handlers/#service-handler","text":"class Service ( ABC ): service_list = None service_path = None def __init_subclass__ ( cls , ** kwargs ) -> None : for required in ( \"service_list\" , \"service_path\" ): if not getattr ( cls , required ): raise TypeError ( f \"Can't instantiate class { cls . __name__ } without { required } attribute defined\" ) return super () . __init_subclass__ ( ** kwargs ) @abstractclassmethod def services ( self , * args , ** kwargs ): return [ \"\" . join ([ self . service_path , service ]) for service in self . service_list ] @abstractclassmethod def execute_service ( self , * args , ** kwargs ): return None","title":"Service Handler"},{"location":"handlers/#log-handler","text":"The log handler control how logs are formatted. from Handlers.env_handler import env class LogHandler : def __init__ ( self , log_file = env ( \"LOG\" , \"ERROR_LOG\" )): # set our database logger self . logger = logging . getLogger ( __name__ ) self . logger . setLevel ( env ( \"LOG\" , \"LOG_LEVEL\" )) # set the log formatter self . formatter = logging . Formatter ( \"------------------------ \\n %(asctime)s \\n ------------------------ \\n %(filename)s : \\n \\ \\n %(message)s \\n @ %(funcName)s : %(pathname)s \" ) # set the file handler self . file_handler = logging . FileHandler ( filename = log_file ) self . file_handler . setFormatter ( self . formatter ) self . logger . addHandler ( self . file_handler )","title":"Log Handler"},{"location":"providers/","text":"Service Providers Import Data Service Provider # scripts/Providers/ImportDataServiceProvider.py class ImportDataServiceProvider ( Service ): # get the data stores from config.ini __import_froms = env ( \"SERVER\" , \"DATA_STORES\" ) def __init__ ( self , service_list : List = None , import_from : str = \"DB\" ) -> None : \"\"\"Imports data from provided source, Database or Warehouse Args: service_list (names of objects to import, optional): _description_. Defaults to None. import_from (str, optional): specifies where to import/download data from DB|WAREHOUSE. Defaults to \"DB\". \"\"\" print ( \"Importing Data...\" ) # validate if import_from is registered in config self . __validate_import_from ( import_from ) self . import_from = import_from . upper () # names of objects to import self . service_list = service_list Export Data Service Provider # scripts/Providers/ImportDataServiceProvider.py class ExportDataServiceProvider ( Service ): # get the data stores from config.ini __upload_to_list = env ( \"SERVER\" , \"DATA_STORES\" ) def __init__ ( self , service_list : List = None , export_to : str = \"DB\" , schema = env ( \"SERVER\" , \"DB_STAGING_SCHEMA\" ), ) -> None : \"\"\"uploads Args: service_list (List): Name of files to upload. upload_to (str, optional): where to upload the files, either 'DB' or 'WAREHOUSE'. Defaults to \"DB\". Raises: TypeError: if upload_to_type is not registered in config.ini \"\"\" print ( \"Uploading Data...\" ) # validate if upload_to type is registered in config self . __validate_upload_to ( upload_to ) self . export_to = export_to . upper () # name of files in data/raw to upload. self . service_list = service_list # name of the postgres schema self . default_schema = schema Analytics Service Provider # scripts/Providers/AnalyticsServiceProvider.py class AnalyticsServiceProvider ( Service ): # name of analytics in /SQL e.g. \"product_analysis.sql\" service_list = [ \"analytics1.sql\" , \"analytics2.sql\" , '...' ] service_path = \"../Data2bot-Assessment/sql\"","title":"Service Providers"},{"location":"providers/#service-providers","text":"","title":"Service Providers"},{"location":"providers/#import-data-service-provider","text":"# scripts/Providers/ImportDataServiceProvider.py class ImportDataServiceProvider ( Service ): # get the data stores from config.ini __import_froms = env ( \"SERVER\" , \"DATA_STORES\" ) def __init__ ( self , service_list : List = None , import_from : str = \"DB\" ) -> None : \"\"\"Imports data from provided source, Database or Warehouse Args: service_list (names of objects to import, optional): _description_. Defaults to None. import_from (str, optional): specifies where to import/download data from DB|WAREHOUSE. Defaults to \"DB\". \"\"\" print ( \"Importing Data...\" ) # validate if import_from is registered in config self . __validate_import_from ( import_from ) self . import_from = import_from . upper () # names of objects to import self . service_list = service_list","title":"Import Data Service Provider"},{"location":"providers/#export-data-service-provider","text":"# scripts/Providers/ImportDataServiceProvider.py class ExportDataServiceProvider ( Service ): # get the data stores from config.ini __upload_to_list = env ( \"SERVER\" , \"DATA_STORES\" ) def __init__ ( self , service_list : List = None , export_to : str = \"DB\" , schema = env ( \"SERVER\" , \"DB_STAGING_SCHEMA\" ), ) -> None : \"\"\"uploads Args: service_list (List): Name of files to upload. upload_to (str, optional): where to upload the files, either 'DB' or 'WAREHOUSE'. Defaults to \"DB\". Raises: TypeError: if upload_to_type is not registered in config.ini \"\"\" print ( \"Uploading Data...\" ) # validate if upload_to type is registered in config self . __validate_upload_to ( upload_to ) self . export_to = export_to . upper () # name of files in data/raw to upload. self . service_list = service_list # name of the postgres schema self . default_schema = schema","title":"Export Data Service Provider"},{"location":"providers/#analytics-service-provider","text":"# scripts/Providers/AnalyticsServiceProvider.py class AnalyticsServiceProvider ( Service ): # name of analytics in /SQL e.g. \"product_analysis.sql\" service_list = [ \"analytics1.sql\" , \"analytics2.sql\" , '...' ] service_path = \"../Data2bot-Assessment/sql\"","title":"Analytics Service Provider"},{"location":"recommendations/","text":"Recommendations This simple pipeline can be further extended to support other kinds of databases such as MySQL, as of today it has only been tested on POSTGRES During the short period of coupling this app, a lot of ideas remain untouched. A","title":"Recommendations"},{"location":"recommendations/#recommendations","text":"This simple pipeline can be further extended to support other kinds of databases such as MySQL, as of today it has only been tested on POSTGRES During the short period of coupling this app, a lot of ideas remain untouched. A","title":"Recommendations"}]}